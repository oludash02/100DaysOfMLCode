{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST classification using ANNs in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Victor\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "(x_train,y_train),(x_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Digit is: 5')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16bfdd1f898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize data\n",
    "plt.imshow(x_train[0])\n",
    "plt.title('Digit is: {}'.format(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((60000),x_train.shape[1]**2)\n",
    "x_test = x_test.reshape((10000),x_test.shape[1]**2)\n",
    "\n",
    "x_train = x_train.astype('float32')/255\n",
    "x_test = x_test.astype('float32')/255\n",
    "\n",
    "#Labels need to be converted to categorical\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784, )))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#Compilation step\n",
    "model.compile(optimizer='adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 92us/step - loss: 0.2689 - acc: 0.9251\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.1090 - acc: 0.9682\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.0711 - acc: 0.9794\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.0510 - acc: 0.9849\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.0367 - acc: 0.9890\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.0275 - acc: 0.9921\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.0203 - acc: 0.9948\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.0152 - acc: 0.9960\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.0122 - acc: 0.9970\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.0098 - acc: 0.9978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16bfdd5af28>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 61us/step\n",
      "Test accuracy: 0.9822 \n",
      "Test loss 0.06234070780656766\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy: {} \\nTest loss {}'.format(test_accuracy, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training same model using Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) #transforms to apply\n",
    "\n",
    "train_data = datasets.MNIST('data/', train=True, transform=all_transform, download=True)\n",
    "test_data = datasets.MNIST('data/', train=False, transform=all_transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=128,shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16b81a0b0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize data\n",
    "def plot_image(img):\n",
    "    img = img.numpy()[0]\n",
    "    mean = 0.1307 #mean of the mnist dataset\n",
    "    std = 0.3081 #standard deviation of the dataset\n",
    "    img = ((mean*img) + std) #recover original image\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "sample_data = next(iter(train_loader)) \n",
    "plot_image(sample_data[0][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 10)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [128/469], Loss: 0.2189\n",
      "Epoch [1/10], Step [256/469], Loss: 0.1027\n",
      "Epoch [1/10], Step [384/469], Loss: 0.1395\n",
      "Epoch [2/10], Step [128/469], Loss: 0.0446\n",
      "Epoch [2/10], Step [256/469], Loss: 0.0847\n",
      "Epoch [2/10], Step [384/469], Loss: 0.0373\n",
      "Epoch [3/10], Step [128/469], Loss: 0.0192\n",
      "Epoch [3/10], Step [256/469], Loss: 0.0156\n",
      "Epoch [3/10], Step [384/469], Loss: 0.1187\n",
      "Epoch [4/10], Step [128/469], Loss: 0.0151\n",
      "Epoch [4/10], Step [256/469], Loss: 0.0738\n",
      "Epoch [4/10], Step [384/469], Loss: 0.0564\n",
      "Epoch [5/10], Step [128/469], Loss: 0.0521\n",
      "Epoch [5/10], Step [256/469], Loss: 0.0138\n",
      "Epoch [5/10], Step [384/469], Loss: 0.0316\n",
      "Epoch [6/10], Step [128/469], Loss: 0.0161\n",
      "Epoch [6/10], Step [256/469], Loss: 0.0108\n",
      "Epoch [6/10], Step [384/469], Loss: 0.0151\n",
      "Epoch [7/10], Step [128/469], Loss: 0.0045\n",
      "Epoch [7/10], Step [256/469], Loss: 0.0119\n",
      "Epoch [7/10], Step [384/469], Loss: 0.0732\n",
      "Epoch [8/10], Step [128/469], Loss: 0.0010\n",
      "Epoch [8/10], Step [256/469], Loss: 0.0085\n",
      "Epoch [8/10], Step [384/469], Loss: 0.0472\n",
      "Epoch [9/10], Step [128/469], Loss: 0.0054\n",
      "Epoch [9/10], Step [256/469], Loss: 0.0035\n",
      "Epoch [9/10], Step [384/469], Loss: 0.0553\n",
      "Epoch [10/10], Step [128/469], Loss: 0.0180\n",
      "Epoch [10/10], Step [256/469], Loss: 0.0164\n",
      "Epoch [10/10], Step [384/469], Loss: 0.0067\n"
     ]
    }
   ],
   "source": [
    "no_epochs = 10\n",
    "steps = len(train_loader)\n",
    "for epoch in range(no_epochs):\n",
    "    for i,(images,labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1,28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs,labels)\n",
    "        \n",
    "        #Backward propagation and optimize\n",
    "        optimizer.zero_grad() #initiliaze gradients\n",
    "        loss.backward() #perform backward propagation\n",
    "        optimizer.step()\n",
    "        \n",
    "        if(i+1) % 128 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch+1, no_epochs, i+1, steps, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97.96 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
